{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44bff903-0e4d-4f3e-a75c-d3cfe8ab4dea",
   "metadata": {},
   "source": [
    "# Linear least squares\n",
    "\n",
    "The solution to a well-posed linear system $Ax = b$ is given by $x = A^{-1}b$. If the matrix is rectangular or not invertible, then we may generalise the notion of solution to $x = A^{\\dagger}b$, where $A^{\\dagger}$ denotes the [Moore--Penrose pseudoinverse](https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse).\n",
    "\n",
    "Lineax can handle problems of this type too.\n",
    "\n",
    "!!! info\n",
    "\n",
    "    For reference: in core JAX, problems of this type are handled using [`jax.numpy.linalg.lstsq`](https://jax.readthedocs.io/en/latest/_autosummary/jax.numpy.linalg.lstsq.html#jax.numpy.linalg.lstsq).\n",
    "    \n",
    "---\n",
    "\n",
    "## Picking a solver\n",
    "\n",
    "By default, the linear solve will fail. This will be a compile-time failure if using a rectangular matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a956c3f2-a70c-472f-9fa9-3dbc16293e1d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot use `AutoLinearSolver(well_posed=True)` with a non-square operator. If you are trying solve a least-squares problem then you should pass `solver=AutoLinearSolver(well_posed=False)`. By default `lineax.linear_solve` assumes that the operator is square and nonsingular.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31mValueError\u001b[0m\u001b[0;31m:\u001b[0m Cannot use `AutoLinearSolver(well_posed=True)` with a non-square operator. If you are trying solve a least-squares problem then you should pass `solver=AutoLinearSolver(well_posed=False)`. By default `lineax.linear_solve` assumes that the operator is square and nonsingular.\n"
     ]
    }
   ],
   "source": [
    "import jax.random as jr\n",
    "import lineax as lx\n",
    "\n",
    "\n",
    "vector = jr.normal(jr.PRNGKey(1), (3,))\n",
    "\n",
    "rectangular_matrix = jr.normal(jr.PRNGKey(0), (3, 4))\n",
    "rectangular_operator = lx.MatrixLinearOperator(rectangular_matrix)\n",
    "lx.linear_solve(rectangular_operator, vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba55c0dd-b696-497a-8b13-896c3a95d5fd",
   "metadata": {},
   "source": [
    "Or it will happen at run time if using a rank-deficient matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0e7ffe6-1e3d-46dc-9dbd-d5ed4c2dedf4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "XlaRuntimeError",
     "evalue": "INTERNAL: Generated function failed: CpuCallback error: RuntimeError: The linear solver returned non-finite (NaN or inf) output. This usually means that the\noperator was not well-posed, and that the solver does not support this.\n\nIf you are trying solve a linear least-squares problem then you should pass\n`solver=AutoLinearSolver(well_posed=False)`. By default `lineax.linear_solve`\nassumes that the operator is square and nonsingular.\n\nIf you *were* expecting this solver to work with this operator, then it may be because:\n\n(a) the operator is singular, and your code has a bug; or\n\n(b) the operator was nearly singular (i.e. it had a high condition number:\n    `jnp.linalg.cond(operator.as_matrix())` is large), and the solver suffered from\n    numerical instability issues; or\n\n(c) the operator is declared to exhibit a certain property (e.g. positive definiteness)\n    that is does not actually satisfy.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31mXlaRuntimeError\u001b[0m\u001b[0;31m:\u001b[0m INTERNAL: Generated function failed: CpuCallback error: RuntimeError: The linear solver returned non-finite (NaN or inf) output. This usually means that the\noperator was not well-posed, and that the solver does not support this.\n\nIf you are trying solve a linear least-squares problem then you should pass\n`solver=AutoLinearSolver(well_posed=False)`. By default `lineax.linear_solve`\nassumes that the operator is square and nonsingular.\n\nIf you *were* expecting this solver to work with this operator, then it may be because:\n\n(a) the operator is singular, and your code has a bug; or\n\n(b) the operator was nearly singular (i.e. it had a high condition number:\n    `jnp.linalg.cond(operator.as_matrix())` is large), and the solver suffered from\n    numerical instability issues; or\n\n(c) the operator is declared to exhibit a certain property (e.g. positive definiteness)\n    that is does not actually satisfy.\n"
     ]
    }
   ],
   "source": [
    "deficient_matrix = jr.normal(jr.PRNGKey(0), (3, 3)).at[0].set(0)\n",
    "deficient_operator = lx.MatrixLinearOperator(deficient_matrix)\n",
    "lx.linear_solve(deficient_operator, vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5cedab-75e5-4b52-88d9-b9d574be7e19",
   "metadata": {},
   "source": [
    "Whilst linear least squares and pseudoinverse are a strict generalisation of linear solves and inverses (respectively), Lineax will *not* attempt to handle the ill-posed case automatically. This is because the algorithms for handling this case are much more computationally expensive.!\n",
    "\n",
    "If your matrix may be rectangular, but is still known to be full rank, then you can set the solver to allow this case like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45abc8bf-4fcf-46be-a91a-58f4e04ac10e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rectangular_solution:  [-0.3214848  -0.75565964 -0.6034579  -0.01326615]\n"
     ]
    }
   ],
   "source": [
    "rectangular_solution = lx.linear_solve(\n",
    "    rectangular_operator, vector, solver=lx.AutoLinearSolver(well_posed=None)\n",
    ")\n",
    "print(\"rectangular_solution: \", rectangular_solution.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86dc9e2f-fe2e-48c8-86ca-bc57f8137246",
   "metadata": {},
   "source": [
    "If your matrix may be either rectangular or rank-deficient, then you can set the solver to all this case like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9a2d92c-3676-471e-bb4a-5fd3b4748fd4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deficient_solution:  [ 0.06046088 -1.0412765   0.8860444 ]\n"
     ]
    }
   ],
   "source": [
    "deficient_solution = lx.linear_solve(\n",
    "    deficient_operator, vector, solver=lx.AutoLinearSolver(well_posed=False)\n",
    ")\n",
    "print(\"deficient_solution: \", deficient_solution.value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b870311-de0f-434c-a9e7-2d8ebf9f0b38",
   "metadata": {},
   "source": [
    "Most users will want to use [`lineax.AutoLinearSolver`][], and not think about the details of which algorithm is selected.\n",
    "\n",
    "If you want to pick a particular algorithm, then that can be done too. [`lineax.QR`][] is capable of handling rectangular full-rank operators, and [`lineax.SVD`][] is capable of handling rank-deficient operators. (And in fact these are the algorithms that `AutoLinearSolver` is selecting in the examples above.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9649746-b0ef-495b-9ea1-eb5f6ca2e7e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Differences from `jax.numpy.linalg.lstsq`?\n",
    "\n",
    "Lineax offers both speed and correctness advantages over the built-in algorithm. (This is partly because the built-in function has to have the same API as NumPy, so JAX is constrained in how it can be implemented.)\n",
    "\n",
    "### Speed (forward)\n",
    "\n",
    "First, in the rectangular case, then the QR algorithm is much faster than the SVD algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d46d0c9a-47e4-439d-9beb-c9aaf47faa5d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX solution: [-0.10002219  0.09477127 -0.10846332 ... -0.08007179 -0.01216239\n",
      " -0.030862  ]\n",
      "Lineax solution: [-0.1000222   0.0947713  -0.10846333 ... -0.08007187 -0.01216241\n",
      " -0.03086199]\n",
      "\n",
      "JAX time: 0.011344402999384329\n",
      "Lineax time: 0.0028611960005946457\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "matrix = jr.normal(jr.PRNGKey(0), (500, 200))\n",
    "vector = jr.normal(jr.PRNGKey(1), (500,))\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def solve_jax(matrix, vector):\n",
    "    out, *_ = jnp.linalg.lstsq(matrix, vector)\n",
    "    return out\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def solve_lineax(matrix, vector):\n",
    "    operator = lx.MatrixLinearOperator(matrix)\n",
    "    solver = lx.QR()  # or lx.AutoLinearSolver(well_posed=None)\n",
    "    solution = lx.linear_solve(operator, vector, solver)\n",
    "    return solution.value\n",
    "\n",
    "\n",
    "solution_jax = solve_jax(matrix, vector)\n",
    "solution_lineax = solve_lineax(matrix, vector)\n",
    "with np.printoptions(threshold=10):\n",
    "    print(\"JAX solution:\", solution_jax)\n",
    "    print(\"Lineax solution:\", solution_lineax)\n",
    "print()\n",
    "time_jax = timeit.repeat(lambda: solve_jax(matrix, vector), number=1, repeat=10)\n",
    "time_lineax = timeit.repeat(lambda: solve_lineax(matrix, vector), number=1, repeat=10)\n",
    "print(\"JAX time:\", min(time_jax))\n",
    "print(\"Lineax time:\", min(time_lineax))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397773d7-f782-45e6-9934-11c62c741380",
   "metadata": {},
   "source": [
    "### Speed (gradients)\n",
    "\n",
    "Lineax also uses a slightly more efficient autodifferentiation implementation, which ensures it is faster, even when both are using the SVD algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1988d0f6-86f5-401a-9615-30cccf04d129",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX gradients: [[-1.75446249e-03  2.00700224e-03 ... -3.16517282e-04 -6.08515576e-04]\n",
      " [ 1.81865180e-04  4.51280124e-04 ... -1.64618701e-04 -6.53692259e-05]\n",
      " ...\n",
      " [-7.27269216e-04  1.27710134e-03 ... -2.64510425e-04 -3.38940619e-04]\n",
      " [ 6.55723223e-03 -3.18011409e-03 ... -1.10758876e-04  1.43246143e-03]]\n",
      "Lineax gradients: [[-1.7544631e-03  2.0070139e-03 ... -3.1653541e-04 -6.0847402e-04]\n",
      " [ 1.8186278e-04  4.5128341e-04 ... -1.6459504e-04 -6.5359738e-05]\n",
      " ...\n",
      " [-7.2721508e-04  1.2771402e-03 ... -2.6450949e-04 -3.3894143e-04]\n",
      " [ 6.5572355e-03 -3.1801097e-03 ... -1.1071599e-04  1.4324478e-03]]\n",
      "\n",
      "JAX time: 0.016591553001489956\n",
      "Lineax time: 0.012212782999995397\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "@jax.grad\n",
    "def grad_jax(matrix):\n",
    "    out, *_ = jnp.linalg.lstsq(matrix, vector)\n",
    "    return out.sum()\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "@jax.grad\n",
    "def grad_lineax(matrix):\n",
    "    operator = lx.MatrixLinearOperator(matrix)\n",
    "    solution = lx.linear_solve(operator, vector, lx.SVD())\n",
    "    return solution.value.sum()\n",
    "\n",
    "\n",
    "gradients_jax = grad_jax(matrix)\n",
    "gradients_lineax = grad_lineax(matrix)\n",
    "with np.printoptions(threshold=10, edgeitems=2):\n",
    "    print(\"JAX gradients:\", gradients_jax)\n",
    "    print(\"Lineax gradients:\", gradients_lineax)\n",
    "print()\n",
    "time_jax = timeit.repeat(lambda: grad_jax(matrix), number=1, repeat=10)\n",
    "time_lineax = timeit.repeat(lambda: grad_lineax(matrix), number=1, repeat=10)\n",
    "print(\"JAX time:\", min(time_jax))\n",
    "print(\"Lineax time:\", min(time_lineax))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a1da5a-3474-4613-926f-5c9d9cdcb4a7",
   "metadata": {},
   "source": [
    "### Correctness (gradients)\n",
    "\n",
    "Core JAX unfortunately has a bug that means it sometimes produces NaN gradients. Lineax does not:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "66b3a08e-92d0-4d0f-a5ea-9e8e5265d259",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX gradients: [[nan nan nan]\n",
      " [nan nan nan]\n",
      " [nan nan nan]]\n",
      "Lineax gradients: [[ 0. -1. -2.]\n",
      " [ 0. -1. -2.]\n",
      " [ 0. -1. -2.]]\n"
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "@jax.grad\n",
    "def grad_jax(matrix):\n",
    "    out, *_ = jnp.linalg.lstsq(matrix, jnp.arange(3.0))\n",
    "    return out.sum()\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "@jax.grad\n",
    "def grad_lineax(matrix):\n",
    "    operator = lx.MatrixLinearOperator(matrix)\n",
    "    solution = lx.linear_solve(operator, jnp.arange(3.0), lx.SVD())\n",
    "    return solution.value.sum()\n",
    "\n",
    "\n",
    "print(\"JAX gradients:\", grad_jax(jnp.eye(3)))\n",
    "print(\"Lineax gradients:\", grad_lineax(jnp.eye(3)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py39",
   "language": "python",
   "name": "py39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
